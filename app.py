import streamlit as st
import cv2
import numpy as np
import torch
import torch.nn as nn
import mediapipe as mp
import pandas as pd
import tempfile
import os

# --- 1. é…ç½®é¡µé¢ ---
st.set_page_config(page_title="Gesture Recognition Demonstration", page_icon="ğŸ–ï¸")
st.title("ğŸ–ï¸ Intelligent gesture recognition system")
st.write("Upload a video, and AI will recognize which type of gesture it is and display the proportion of all possibilities.")

# --- 2. å®šä¹‰æ¨¡å‹æ¶æ„ ---
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attention = nn.Linear(hidden_dim, 1)

    def forward(self, lstm_output):
        energy = self.attention(lstm_output)
        weights = torch.softmax(energy, dim=1)
        context_vector = torch.sum(lstm_output * weights, dim=1)
        return context_vector, weights

class BiLSTMAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, num_layers=2):
        super(BiLSTMAttention, self).__init__()
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers=num_layers,
            batch_first=True, dropout=0.4, bidirectional=True
        )
        self.attention = Attention(hidden_size * 2)
        self.bn = nn.BatchNorm1d(hidden_size * 2)
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)
        self.output_layer = nn.Linear(128, num_classes)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        context_vector, _ = self.attention(lstm_out)
        out = self.bn(context_vector)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.output_layer(out)
        return out

# --- 3. å·¥å…·å‡½æ•° ---
mp_holistic = mp.solutions.holistic

def extract_keypoints(results):
    pose = np.array([[r.x, r.y, r.z, r.visibility] for r in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    lh = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, lh, rh])

# --- 4. åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    # è¿™é‡Œå®šä¹‰ä½ çš„15ä¸ªæ‰‹åŠ¿æ ‡ç­¾
    # è¿™é‡Œæˆ‘æš‚æ—¶ç”¨ User 1 åˆ° 15 ä»£æ›¿ï¼Œéœ€è¦æ ¹æ® BIM æ–‡ä»¶å¤¹ä¿®æ”¹
    gestures = [f"Gesture {i}" for i in range(1, 16)] 
    
    device = torch.device("cpu")
    model = BiLSTMAttention(input_size=258, hidden_size=128, num_classes=len(gestures))
    
    try:
        model.load_state_dict(torch.load("trained_model.pth", map_location=device))
        model.eval()
    except FileNotFoundError:
        st.error("The model file trained_model.pth cannot be found. Please ensure that it has been uploaded.")
        return None, None
        
    return model, gestures

model, gestures = load_model()

# --- 5. è§†é¢‘å¤„ç†é€»è¾‘ ---
uploaded_file = st.file_uploader("Please upload the video file (mp4, mov, avi)", type=['mp4', 'mov', 'avi'])

if uploaded_file is not None and model is not None:
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ä¾›OpenCVè¯»å–
    tfile = tempfile.NamedTemporaryFile(delete=False) 
    tfile.write(uploaded_file.read())
    
    # æ˜¾ç¤ºè§†é¢‘
    st.video(uploaded_file)
    
    if st.button("Start recognition"):
        with st.spinner("Analyzing gestures frame by frame..."):
            cap = cv2.VideoCapture(tfile.name)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            skip = max(int(total_frames / 30), 1)
            
            sequence = []
            
            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
                for i in range(30):
                    cap.set(cv2.CAP_PROP_POS_FRAMES, i * skip)
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    res = holistic.process(frame)
                    sequence.append(extract_keypoints(res))
            
            cap.release()
            
            # è¡¥é½å¸§æ•°
            while len(sequence) < 30:
                sequence.append(np.zeros(258))
                
            # æ¨ç†
            input_tensor = torch.tensor(np.array([sequence]), dtype=torch.float32)
            
            with torch.no_grad():
                output = model(input_tensor)
                probs = torch.softmax(output, dim=1)[0] # è·å–æ¦‚ç‡åˆ†å¸ƒ
                
            # --- 6. ç»“æœå±•ç¤º ---
            # è·å–æœ€é«˜ç½®ä¿¡åº¦çš„ç»“æœ
            conf, idx = torch.max(probs, 0)
            predicted_gesture = gestures[idx.item()]
            confidence_score = conf.item() * 100
            
            st.success(f"Recognition result: **{predicted_gesture}**")
            st.info(f"Confidence level: **{confidence_score:.2f}%**")
            
            # å±•ç¤ºæ‰€æœ‰15ä¸ªåŠ¨ä½œçš„æ¯”é‡ï¼ˆæŸ±çŠ¶å›¾ï¼‰
            st.write("### Action probability distribution")
            
            # æ„å»ºDataFrameç”¨äºå›¾è¡¨
            probs_np = probs.numpy()
            chart_data = pd.DataFrame({
                "Gesture": gestures,
                "Probability": probs_np
            }).sort_values(by="Probability", ascending=False)
            
            # äº¤äº’å¼æŸ±çŠ¶å›¾
            st.bar_chart(chart_data, x="Gesture", y="Probability", color="#FF4B4B")