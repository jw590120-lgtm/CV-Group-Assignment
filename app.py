import streamlit as st
import cv2
import numpy as np
import torch
import torch.nn as nn
import mediapipe as mp
import pandas as pd
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode
import av
import threading
import gc

# ===========================
# 1. åŸºç¡€é…ç½®
# ===========================
st.set_page_config(page_title="AI Gesture Lite", page_icon="ğŸ–ï¸")

# å…¼å®¹æ€§è¡¥ä¸
if not hasattr(st, "experimental_rerun"):
    st.experimental_rerun = st.rerun

# ===========================
# 2. è½»é‡åŒ–æ¨¡å‹å®šä¹‰
# ===========================
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

# ç®€åŒ–çš„æ¨¡å‹ç»“æ„ï¼ˆåªç”¨äºæ¨ç†ï¼‰
class BiLSTMAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(BiLSTMAttention, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes) # ç®€åŒ–å…¨è¿æ¥å±‚

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œç›´æ¥å–æœ€åä¸€å¸§ï¼ŒèŠ‚çœè®¡ç®—é‡
        out = self.fc(lstm_out[:, -1, :])
        return out

def extract_keypoints(results):
    # ä¿æŒæ•°æ®æ ¼å¼ä¸€è‡´
    pose = np.array([[r.x, r.y, r.z, r.visibility] for r in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    lh = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, lh, rh])

@st.cache_resource
def load_model():
    gestures = [f"Gesture {i}" for i in range(1, 16)]
    try:
        class Attention(nn.Module):
            def __init__(self, hidden_dim):
                super(Attention, self).__init__()
                self.attention = nn.Linear(hidden_dim, 1)
            def forward(self, lstm_output):
                energy = self.attention(lstm_output)
                weights = torch.softmax(energy, dim=1)
                context_vector = torch.sum(lstm_output * weights, dim=1)
                return context_vector, weights

        class OriginalBiLSTM(nn.Module):
            def __init__(self, input_size, hidden_size, num_classes, num_layers=2):
                super(OriginalBiLSTM, self).__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=0.4, bidirectional=True)
                self.attention = Attention(hidden_size * 2)
                self.bn = nn.BatchNorm1d(hidden_size * 2)
                self.fc1 = nn.Linear(hidden_size * 2, 128)
                self.relu = nn.ReLU()
                self.dropout = nn.Dropout(0.4)
                self.output_layer = nn.Linear(128, num_classes)
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                context_vector, _ = self.attention(lstm_out)
                out = self.bn(context_vector)
                out = self.fc1(out)
                out = self.relu(out)
                out = self.dropout(out)
                out = self.output_layer(out)
                return out

        device = torch.device("cpu")
        model = OriginalBiLSTM(input_size=258, hidden_size=128, num_classes=len(gestures))
        model.load_state_dict(torch.load("trained_model.pth", map_location=device))
        model.eval()
        return model, gestures
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None, None

global_model, global_gestures = load_model()

# ===========================
# 3. æé€Ÿç‰ˆå¤„ç†å™¨
# ===========================
class GestureProcessor(VideoProcessorBase):
    def __init__(self):
        # âš ï¸ å…³é”®ä¼˜åŒ–ï¼šmodel_complexity=0 (æœ€å¿«ï¼Œæœ€çœå†…å­˜)
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            model_complexity=0, 
            refine_face_landmarks=False
        )
        self.sequence = []
        self.predicted_gesture = "Init..."
        self.lock = threading.Lock()

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        try:
            image = frame.to_ndarray(format="bgr24")
            
            # âš ï¸ å…³é”®ä¼˜åŒ–ï¼šå¼ºåˆ¶ç¼©å°åˆ†è¾¨ç‡
            image = cv2.resize(image, (320, 240)) 

            image.flags.writeable = False
            results = self.holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            image.flags.writeable = True

            # ä»…ç»˜åˆ¶æ‰‹éƒ¨ï¼Œå‡å°‘ CPU ç»˜å›¾å‹åŠ›
            if results.left_hand_landmarks:
                mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            if results.right_hand_landmarks:
                mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

            keypoints = extract_keypoints(results)
            
            with self.lock:
                self.sequence.append(keypoints)
                self.sequence = self.sequence[-30:]

                if len(self.sequence) == 30 and global_model:
                    inp = torch.tensor(np.array([self.sequence]), dtype=torch.float32)
                    with torch.no_grad():
                        out = global_model(inp)
                        probs = torch.softmax(out, dim=1)[0]
                        conf, idx = torch.max(probs, 0)
                        if conf.item() > 0.5:
                            self.predicted_gesture = f"{global_gestures[idx.item()]} ({int(conf.item()*100)}%)"
                        else:
                            self.predicted_gesture = "..."

            cv2.putText(image, self.predicted_gesture, (10, 30), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # âš ï¸ å…³é”®ï¼šæ‰‹åŠ¨åƒåœ¾å›æ”¶
            gc.collect()
            
            return av.VideoFrame.from_ndarray(image, format="bgr24")
        except Exception:
            return frame

# ===========================
# 4. ç•Œé¢
# ===========================
st.title("ğŸ–ï¸ Gesture Recognition (Lite)")
st.caption("Running in Low-Latency Mode for Free Tier Servers")

rtc_configuration = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# âš ï¸ å…³é”®ä¼˜åŒ–ï¼šå…³é—­ async_processingï¼Œé˜²æ­¢é˜Ÿåˆ—å †ç§¯å†…å­˜æº¢å‡º
webrtc_ctx = webrtc_streamer(
    key="gesture-recognition-lite",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=rtc_configuration,
    video_processor_factory=GestureProcessor,
    media_stream_constraints={"video": True, "audio": False},
    async_processing=False 
)

if webrtc_ctx.state.playing:
    st.success("Camera Active")
