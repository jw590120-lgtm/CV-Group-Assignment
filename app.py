import streamlit as st
import cv2
import numpy as np
import torch
import torch.nn as nn
import mediapipe as mp
import pandas as pd
import tempfile
import time
import os

# ===========================
# 1. é¡µé¢é…ç½®ä¸ç¾åŒ– (UI Configuration)
# ===========================
st.set_page_config(
    page_title="AI Gesture Studio",
    page_icon="ğŸ¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
    <style>
    .main {
        background-color: #f8f9fa;
    }
    .stButton>button {
        width: 100%;
        border-radius: 10px;
        height: 3em;
        background-color: #FF4B4B;
        color: white;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #D93F3F;
        border-color: #D93F3F;
    }
    h1 {
        color: #1E1E1E;
    }
    .css-1aumxhk {
        padding: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)

# ===========================
# 2. æ ¸å¿ƒæ¨¡å‹å®šä¹‰ (Model Core)
# ===========================
mp_holistic = mp.solutions.holistic

class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attention = nn.Linear(hidden_dim, 1)
    def forward(self, lstm_output):
        energy = self.attention(lstm_output)
        weights = torch.softmax(energy, dim=1)
        context_vector = torch.sum(lstm_output * weights, dim=1)
        return context_vector, weights

class BiLSTMAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, num_layers=2):
        super(BiLSTMAttention, self).__init__()
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers=num_layers,
            batch_first=True, dropout=0.4, bidirectional=True
        )
        self.attention = Attention(hidden_size * 2)
        self.bn = nn.BatchNorm1d(hidden_size * 2)
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)
        self.output_layer = nn.Linear(128, num_classes)
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        context_vector, _ = self.attention(lstm_out)
        out = self.bn(context_vector)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.output_layer(out)
        return out

def extract_keypoints(results):
    pose = np.array([[r.x, r.y, r.z, r.visibility] for r in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    lh = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, lh, rh])

# --- åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    # ã€é‡è¦ã€‘è¯·ä¿®æ”¹ä¸ºä½ çœŸå®çš„15ä¸ªè‹±æ–‡æ‰‹åŠ¿åç§°
    gestures = [f"Gesture {i}" for i in range(1, 16)] 
    
    device = torch.device("cpu")
    model = BiLSTMAttention(input_size=258, hidden_size=128, num_classes=len(gestures))
    
    status_text = "Checking model file..."
    try:
        model.load_state_dict(torch.load("trained_model.pth", map_location=device))
        model.eval()
        return model, gestures, "Loaded"
    except FileNotFoundError:
        return None, None, "Missing File"
    except Exception as e:
        return None, None, f"Error: {str(e)}"

# ===========================
# 3. ä¾§è¾¹æ è®¾è®¡ (Sidebar)
# ===========================
with st.sidebar:
    st.title("ğŸ§© System Dashboard")
    st.markdown("---")
    
    # æ¨¡å‹çŠ¶æ€æŒ‡ç¤ºå™¨
    model, gestures, status = load_model()
    if status == "Loaded":
        st.success("Model Status: **Active** âœ…")
        st.caption(f"Architecture: BiLSTM + Attention\nClasses: {len(gestures)}")
    else:
        st.error(f"Model Status: **{status}** âŒ")
        st.warning("Please upload 'trained_model.pth' to your GitHub repository.")
    
    st.markdown("---")
    st.info("""
    **How to use:**
    1. Upload a video file.
    2. Click 'Start Analysis'.
    3. View frame-by-frame processing.
    4. Check the prediction report.
    """)
    st.markdown("---")
    st.caption("CV Group Assignment 2025")

# ===========================
# 4. ä¸»ç•Œé¢è®¾è®¡ (Main Interface)
# ===========================

# æ ‡é¢˜åŒº
st.markdown("# ğŸ¬ AI Gesture Analysis Studio")
st.markdown("#### Upload a video to identify dynamic gestures using Deep Learning.")
st.markdown("---")

# æ–‡ä»¶ä¸Šä¼ åŒº
uploaded_file = st.file_uploader("", type=['mp4', 'mov', 'avi'], help="Supported formats: MP4, MOV, AVI")

if uploaded_file is not None:
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    tfile = tempfile.NamedTemporaryFile(delete=False)
    tfile.write(uploaded_file.read())
    
    # å¸ƒå±€ï¼šå·¦ä¾§è§†é¢‘ï¼Œå³ä¾§ç»“æœå ä½
    col_video, col_results = st.columns([1.5, 1])
    
    with col_video:
        st.subheader("ğŸ“º Video Preview")
        st.video(uploaded_file)
        
        # å¯åŠ¨æŒ‰é’®
        process_btn = st.button("ğŸš€ Start Deep Analysis", type="primary")

    if process_btn:
        if model is None:
            st.error("Cannot proceed: Model not loaded.")
        else:
            with col_results:
                st.subheader("ğŸ“Š Analysis Report")
                
                # è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # --- è§†é¢‘å¤„ç†é€»è¾‘ ---
                cap = cv2.VideoCapture(tfile.name)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                
                if total_frames == 0: total_frames = 100
                
                # é‡‡æ ·ç­–ç•¥ï¼šå‡åŒ€æå–30å¸§
                skip = max(int(total_frames / 30), 1)
                sequence = []
                
                status_text.markdown("**ğŸ”„ Initializing MediaPipe...**")
                
                # ä½¿ç”¨ MediaPipe å¤„ç†
                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
                    frames_processed = 0
                    
                    for i in range(30):
                        # æ›´æ–°è¿›åº¦æ¡
                        progress = int((i / 30) * 100)
                        progress_bar.progress(progress)
                        status_text.text(f"Processing frame {i+1}/30...")
                        
                        cap.set(cv2.CAP_PROP_POS_FRAMES, i * skip)
                        ret, frame = cap.read()
                        if not ret: break
                        
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        res = holistic.process(frame)
                        sequence.append(extract_keypoints(res))
                        frames_processed += 1
                
                cap.release()
                progress_bar.progress(100)
                status_text.success("âœ… Feature Extraction Complete!")
                
                # è¡¥é½æ•°æ® (Padding)
                while len(sequence) < 30:
                    sequence.append(np.zeros(258))
                
                # --- æ¨ç†é€»è¾‘ ---
                with st.spinner("ğŸ§  Running Neural Network Inference..."):
                    input_tensor = torch.tensor(np.array([sequence]), dtype=torch.float32)
                    with torch.no_grad():
                        output = model(input_tensor)
                        probs = torch.softmax(output, dim=1)[0]
                    
                    # è·å–ç»“æœ
                    conf, idx = torch.max(probs, 0)
                    prediction = gestures[idx.item()]
                    confidence_val = conf.item() * 100
                    
                    time.sleep(0.5)

                # --- ç»“æœå±•ç¤º (Result Dashboard) ---
                st.divider()
                
                # 1. æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡
                st.metric(
                    label="ğŸ† Top Prediction",
                    value=prediction,
                    delta=f"{confidence_val:.2f}% Confidence"
                )
                
                if confidence_val > 80:
                    st.balloons() 
                
                # 2. æ¦‚ç‡åˆ†å¸ƒå›¾ (æ˜¾ç¤ºæ‰€æœ‰æƒé‡)
                st.write("### ğŸ“ˆ Full Probability Distribution")
                
                # æ•´ç†æ•°æ®
                chart_data = pd.DataFrame({
                    "Gesture": gestures,
                    "Probability": probs.numpy()
                }).sort_values(by="Probability", ascending=False)
                
                # ç›´æ¥å±•ç¤ºæ‰€æœ‰æ•°æ®
                st.bar_chart(
                    chart_data, 
                    x="Gesture", 
                    y="Probability",
                    color="#FF4B4B"
                )
                
                # 3. è¯¦ç»†æ•°æ®å±•å¼€
                with st.expander("ğŸ“„ View Raw Data Table"):
                    st.dataframe(chart_data.style.format({"Probability": "{:.4%}"}))

else:
    # ç©ºçŠ¶æ€æç¤º
    st.info("ğŸ‘ˆ Please upload a video file from the sidebar or main area to begin.")
